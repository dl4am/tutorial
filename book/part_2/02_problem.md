# Problem Formulation

Generally, we consider that an automatic mixing system will take as input a set of $N$ recordings $\mathbf{x_1}, \mathbf{x}_2, ..., \mathbf{x}_N$ each containing $T$ samples. These recordings, when processed and combined, will create the final stereophonic mixture $\hat{Y}$. In popular music, the input recordings generally contain isolated recordings of the individual instruments that make up the composition (e.g. vocals, guitar, bass, drums, etc.). In other genres, these recordings may come from microphones placed close to sources within an orchestra or ensemble while they were performing. Automatic mixing systems also have applications outside of music, so recordings may constitute multiple speakers in a podcast or the dialogue, sound effects, and score of a film.

As discussed in Section 1, the process of creating these constituent recordings is largely unstructured, meaning that different productions can have vastly different recordings in terms of the number of sources and their identity. The task of an automatic mixing system involves analyzing and then manipulating these individual recordings to create a mixture similar to a trained audio engineer. Due to these complexities, there are a number of challenges that arise in constructing an automatic system that can learn from data, which are addressed by systems in different ways.

```{figure} /assets/figures/types-of-systems.svg
:name: types-of-systems
:alt: Types of systems
:align: center

Three formulations for deep learning based automatic mixing systems.
```

The problem formulation adopted by a particular approach depends primarily on the form of the data that will be used for training the automatic mixing system. The two predominant formulations can be described as **direct transformation**, where the input recordings are directly mapped to the output mixture (shown in {numref}`types-of-systems`a), or **parameter estimation**, where the parameters of a mixing console are estimated by a model and then used in downstream mixing console to produce a mix, as shown in Figure {numref}`types-of-systems`b and c. 

There are two general forms of parameter estimation that depend on the form of the dataset and loss function used. When a dataset of both input recordings and parameter is available we can compute the loss in the parameter space (shown in {numref}`types-of-systems`b) and when parameters not available, only the final mixtures, we can instead compute the loss in the audio domain (shown in {numref}`types-of-systems`c). However the second form requires that the mixing console $h$ is differentiable. We will introduce these formulations in the following subsections in a general sense and discuss their tradeoffs. Then in Section 2.3 we will discuss the Mix-Wave-U-Net and Differentiable Mixing Console, which are examples of these two different formulations.

## Direct Transformation

In the direct transformation approach we consider a dataset of $E$ examples $\mathcal{D}_{i=1}^E  \{  \mathbf{x}_{i,1}, \mathbf{x}_{i,2}, ..., \mathbf{x}_{i,N}, \mathbf{Y}_i \}$ each containing $N_i$ recordings $\mathbf{x}_{i,1}, \mathbf{x}_{i,2}, ..., \mathbf{x}_{i,N}$ along with a mixture of these recordings $\mathbf{Y}_i$ created by an audio engineer. We do not need knowledge of the underlying signal processing devices that were used (audio effects, digital audio workstation, etc.) in this approach. We construct a model $f_\theta$ that takes as input the recordings and is trained to produce an estimate of the mix $\hat{\mathbf{Y}}_i$ from the dataset that corresponds to the input tracks $\mathbf{Y}_i$. This involves optimizing the parameters of the model $\bm{\theta}$ according to a loss operating on the audio signals $\mathcal{L}_a$ (in either the time or frequency domains, or both). This formulation makes limited assumptions and depending on the flexibility of $f_\theta$ can enable learning to reproduce common audio effects without any direct supervision or knowledge of these devices beyond the mapping between the input recordings and mixes.

While this approach is desirable due to its limited assumptions and flexibility, this comes with some potential drawbacks. One limitation arises from this flexibility, which can lead to the need for large scale datasets for satisfactory training and generalization. This is of particular importance since in general the availability of even unstructured multitrack data is quite limited. Furthermore, this flexibility can also lead to undesirable signal transformations, namely the introduction of artifacts, which are highly undesirable in the audio engineering context. In addition, both interpretability and controllability are limited in this approach. Users of a system of this nature are unable to both see the control parameters that give rise to the final output mix, and more importantly, they are unable to adapt or modify this mix using traditional control parameters, potentially limiting the utility of the system.

## Parameter Estimation

The limitations of direct transformation approaches motivate parameter estimation approaches, which instead construct a model $f_\theta$ that estimates the parameters of a particular mixing console. This mixing console is described by some function $h$ takes a set of $N$ input recordings and along with parameters for each recording to produce a final stereo mixture $\mathbf{Y} = h(\mathbf{x}_{1}, \mathbf{x}_{2}, ..., \mathbf{x}_{N}, \mathbf{p}_1, \mathbf{p}_2, ..., \mathbf{p}_N)$. In most configurations we assume that the signal processing chain operating on each input is the made of the same building blocks (e.g equaliser, dynamic range compressor, reverberation, etc.) and therefore the parameters for each input $\mathbf{p}_i$ can all be represented by an $N \times K$ matrix $\mathbf{P}$ where $N$ is the number of inputs and $K$ is the number of control parameters for each instance of the signal chain.

Is should be noted that this framing limits the expressivity of the system as compared to the direct transformation approach since we are limited to the capabilities of $h$ when creating a mix. However, it reduces the work that must be done by the model $f_\theta$, since manipulation of the input recordings to create the mixture is handled by a predefined set of signal processing devices described by $h$.

### Parameter Loss

Given a dataset $\mathcal{D}_{i=1}^E \{ \mathbf{x}_{i,1}, \mathbf{x}_{i,2}, ..., \mathbf{x}_{i,N}, \mathbf{p}_{i,1}, \mathbf{p}_{i,2}, ..., \mathbf{p}_{i,N} \}$ that contains both the $N$ input recordings along with the $K$ parameters of a mixing console for each input selected by an audio engineer that will produce a mix of the recordings, we could design a model that learns to estimate these parameters computing a loss in the parameter space. This formulation is shown in Figure []b, where a model $f_\theta$ is trained to estimate control parameters $\hat{\mathbf{P}}$. The weights of the network $\theta$ are then trained according to a loss function $\mathcal{L}_p(\hat{\mathbf{P}}, \mathbf{P})$ that measures the distance (e.g. mean squared error) between the estimated parameters and the ground truth parameters from the dataset. While this is a potentially desirable formulation since it provides a straightforward training setup, it is problematic for two main reasons.

*Datasets*: Foremost is the reality that data of this form is not widely available. While it is possible to collect data of this form, it is challenging due to the diversity of the tools utilized by audio engineers. We could export the settings that describe the mix from digital audio workstation projects, but due to the wide range of digital audio workstations and the audio effect implementations present (plugins), defining a singular ontology onto which all mixing parameters can be mapped will require significant effort and is likely to face many edge cases. 

For example, consider an equaliser effect. This effect is commonly used, however there are many different implementations of an equaliser. Some implementations may feature **CONTINUE**. Therefore, the only route towards constructing a dataset of this form would likely require defining a fixed mixing chain and asking audio engineers to use this to generate training data. However, this results in a significant compromise in data realism since audio engineers often utilize their preferred set of audio effects in different contexts and forcing the use of not only specific effect implementations but also a specific signal processing further limits the data realism.

*Overparameterization:* In addition, another consideration for parameter estimation approach is the potential overparameterization of the audio effects and mixing console. An audio effect or signal processing device is overparameterized when there exists many different configurations in the parameter space of the device that result in outputs that are very close in the audio domain. This could be close in the audio domain with respect to an established objective metric, such as the time domain error or spectral distance, but also in the perceptual space, which is significantly harder to measure, but more likely to cause effects to be overparameterized. This is an issue when using a loss in the parameter space since it is possible for the model to generate parameter predictions that produce a mix close to or similar to the ground truth mix from the dataset but are far apart in the parameter space, and hence penalized during training.

### Audio Loss

As a result, existing parameter estimation systems for automatic mixing generally utilize a dataset that contains only input recordings and the stereo mix created by an audio engineer, without any knowledge of the underlying effect implementation or parameter configurations that were used. This approach uses the same dataset formulation as in the direct transformation approach, which addresses the limitations of the parameter space loss, however it introduces a

Training then involves that process of learning to map the input recordings to a mix that is as close as possible to the target mix in the dataset according to a predefined error metric $\mathcal{L}(\hat{y}, y)$. Often this error metric is the $L_1$ or $L_2$ distance between the mixes in the time domain, or the distance between the magnitude spectrum of the mixes. We will discuss loss functions further in Section 2.4.

