
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Problem Formulation &#8212; Deep Learning for Automatic Mixing</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Methods" href="03_methods.html" />
    <link rel="prev" title="Intelligent Music Production" href="01_imp.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for Automatic Mixing</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing-page.html">
                    Deep Learning for Automatic Mixing 
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Audio Engineering
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../part_1/01_music-production.html">
   Music Production
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_1/music-production/01_mixing.html">
     Mixing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_1/music-production/02_equalization.html">
     Equalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_1/music-production/03_reveberation.html">
     Reverberation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_1/music-production/04_compression.html">
     Compression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../part_1/music-production/05_aesthetics.html">
     Aesthetics
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Automatic Mixing
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_imp.html">
   Intelligent Music Production
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Problem Formulation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="03_methods.html">
   Methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="methods/01_mixwaveunet.html">
     Mix-Wave-U-Net
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="methods/02_dmc.html">
     Differentiable Mixing Console
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_loss-functions.html">
   Loss Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_diffsp.html">
   Differentiable signal processing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Implementation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part_3/01_inference.html">
   Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part_3/02_datasets.html">
   Datasets for automix systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part_3/03_models.html">
   Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part_3/04_training.html">
   Training
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Evaluation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part_4/01_metrics.html">
   Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part_4/05_evaluate.html">
   Evaluation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conclusion
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../part_5/01_future-directions.html">
   Future Directions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part_5/02_conclusion.html">
   Conclusions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../part_5/03_references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/dl4am/tutorial"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/dl4am/tutorial/issues/new?title=Issue%20on%20page%20%2Fpart_2/02_problem.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/part_2/02_problem.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#direct-transformation">
   Direct Transformation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-estimation">
   Parameter Estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-loss">
     Parameter Loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#audio-loss">
     Audio Loss
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Problem Formulation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#direct-transformation">
   Direct Transformation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameter-estimation">
   Parameter Estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameter-loss">
     Parameter Loss
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#audio-loss">
     Audio Loss
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="problem-formulation">
<h1>Problem Formulation<a class="headerlink" href="#problem-formulation" title="Permalink to this headline">#</a></h1>
<p>Generally, we consider that an automatic mixing system will take as input a set of <span class="math notranslate nohighlight">\(N\)</span> recordings <span class="math notranslate nohighlight">\(\mathbf{x_1}, \mathbf{x}_2, ..., \mathbf{x}_N\)</span> each containing <span class="math notranslate nohighlight">\(T\)</span> samples. These recordings, when processed and combined, will create the final stereophonic mixture <span class="math notranslate nohighlight">\(\hat{Y}\)</span>. In popular music, the input recordings generally contain isolated recordings of the individual instruments that make up the composition (e.g. vocals, guitar, bass, drums, etc.). In other genres, these recordings may come from microphones placed close to sources within an orchestra or ensemble while they were performing. Automatic mixing systems also have applications outside of music, so recordings may constitute multiple speakers in a podcast or the dialogue, sound effects, and score of a film.</p>
<p>As discussed in Section 1, the process of creating these constituent recordings is largely unstructured, meaning that different productions can have vastly different recordings in terms of the number of sources and their identity. The task of an automatic mixing system involves analyzing and then manipulating these individual recordings to create a mixture similar to a trained audio engineer. Due to these complexities, there are a number of challenges that arise in constructing an automatic system that can learn from data, which are addressed by systems in different ways.</p>
<figure class="align-center" id="types-of-systems">
<img alt="Types of systems" src="../_images/types-of-systems.svg" /><figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Three formulations for deep learning based automatic mixing systems.</span><a class="headerlink" href="#types-of-systems" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The problem formulation adopted by a particular approach depends primarily on the form of the data that will be used for training the automatic mixing system. The two predominant formulations can be described as <strong>direct transformation</strong>, where the input recordings are directly mapped to the output mixture (shown in <a class="reference internal" href="#types-of-systems"><span class="std std-numref">Fig. 2</span></a>a), or <strong>parameter estimation</strong>, where the parameters of a mixing console are estimated by a model and then used in downstream mixing console to produce a mix, as shown in Figure <a class="reference internal" href="#types-of-systems"><span class="std std-numref">Fig. 2</span></a>b and c.</p>
<p>There are two general forms of parameter estimation that depend on the form of the dataset and loss function used. When a dataset of both input recordings and parameter is available we can compute the loss in the parameter space (shown in <a class="reference internal" href="#types-of-systems"><span class="std std-numref">Fig. 2</span></a>b) and when parameters not available, only the final mixtures, we can instead compute the loss in the audio domain (shown in <a class="reference internal" href="#types-of-systems"><span class="std std-numref">Fig. 2</span></a>c). However the second form requires that the mixing console <span class="math notranslate nohighlight">\(h\)</span> is differentiable. We will introduce these formulations in the following subsections in a general sense and discuss their tradeoffs. Then in Section 2.3 we will discuss the Mix-Wave-U-Net and Differentiable Mixing Console, which are examples of these two different formulations.</p>
<section id="direct-transformation">
<h2>Direct Transformation<a class="headerlink" href="#direct-transformation" title="Permalink to this headline">#</a></h2>
<p>In the direct transformation approach we consider a dataset of <span class="math notranslate nohighlight">\(E\)</span> examples <span class="math notranslate nohighlight">\(\mathcal{D}_{i=1}^E  \{  \mathbf{x}_{i,1}, \mathbf{x}_{i,2}, ..., \mathbf{x}_{i,N}, \mathbf{Y}_i \}\)</span> each containing <span class="math notranslate nohighlight">\(N_i\)</span> recordings <span class="math notranslate nohighlight">\(\mathbf{x}_{i,1}, \mathbf{x}_{i,2}, ..., \mathbf{x}_{i,N}\)</span> along with a mixture of these recordings <span class="math notranslate nohighlight">\(\mathbf{Y}_i\)</span> created by an audio engineer. We do not need knowledge of the underlying signal processing devices that were used (audio effects, digital audio workstation, etc.) in this approach. We construct a model <span class="math notranslate nohighlight">\(f_\theta\)</span> that takes as input the recordings and is trained to produce an estimate of the mix <span class="math notranslate nohighlight">\(\hat{\mathbf{Y}}_i\)</span> from the dataset that corresponds to the input tracks <span class="math notranslate nohighlight">\(\mathbf{Y}_i\)</span>. This involves optimizing the parameters of the model <span class="math notranslate nohighlight">\(\bm{\theta}\)</span> according to a loss operating on the audio signals <span class="math notranslate nohighlight">\(\mathcal{L}_a\)</span> (in either the time or frequency domains, or both). This formulation makes limited assumptions and depending on the flexibility of <span class="math notranslate nohighlight">\(f_\theta\)</span> can enable learning to reproduce common audio effects without any direct supervision or knowledge of these devices beyond the mapping between the input recordings and mixes.</p>
<p>While this approach is desirable due to its limited assumptions and flexibility, this comes with some potential drawbacks. One limitation arises from this flexibility, which can lead to the need for large scale datasets for satisfactory training and generalization. This is of particular importance since in general the availability of even unstructured multitrack data is quite limited. Furthermore, this flexibility can also lead to undesirable signal transformations, namely the introduction of artifacts, which are highly undesirable in the audio engineering context. In addition, both interpretability and controllability are limited in this approach. Users of a system of this nature are unable to both see the control parameters that give rise to the final output mix, and more importantly, they are unable to adapt or modify this mix using traditional control parameters, potentially limiting the utility of the system.</p>
</section>
<section id="parameter-estimation">
<h2>Parameter Estimation<a class="headerlink" href="#parameter-estimation" title="Permalink to this headline">#</a></h2>
<p>The limitations of direct transformation approaches motivate parameter estimation approaches, which instead construct a model <span class="math notranslate nohighlight">\(f_\theta\)</span> that estimates the parameters of a particular mixing console. This mixing console is described by some function <span class="math notranslate nohighlight">\(h\)</span> takes a set of <span class="math notranslate nohighlight">\(N\)</span> input recordings and along with parameters for each recording to produce a final stereo mixture <span class="math notranslate nohighlight">\(\mathbf{Y} = h(\mathbf{x}_{1}, \mathbf{x}_{2}, ..., \mathbf{x}_{N}, \mathbf{p}_1, \mathbf{p}_2, ..., \mathbf{p}_N)\)</span>. In most configurations we assume that the signal processing chain operating on each input is the made of the same building blocks (e.g equaliser, dynamic range compressor, reverberation, etc.) and therefore the parameters for each input <span class="math notranslate nohighlight">\(\mathbf{p}_i\)</span> can all be represented by an <span class="math notranslate nohighlight">\(N \times K\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of inputs and <span class="math notranslate nohighlight">\(K\)</span> is the number of control parameters for each instance of the signal chain.</p>
<p>Is should be noted that this framing limits the expressivity of the system as compared to the direct transformation approach since we are limited to the capabilities of <span class="math notranslate nohighlight">\(h\)</span> when creating a mix. However, it reduces the work that must be done by the model <span class="math notranslate nohighlight">\(f_\theta\)</span>, since manipulation of the input recordings to create the mixture is handled by a predefined set of signal processing devices described by <span class="math notranslate nohighlight">\(h\)</span>.</p>
<section id="parameter-loss">
<h3>Parameter Loss<a class="headerlink" href="#parameter-loss" title="Permalink to this headline">#</a></h3>
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{D}_{i=1}^E \{ \mathbf{x}_{i,1}, \mathbf{x}_{i,2}, ..., \mathbf{x}_{i,N}, \mathbf{p}_{i,1}, \mathbf{p}_{i,2}, ..., \mathbf{p}_{i,N} \}\)</span> that contains both the <span class="math notranslate nohighlight">\(N\)</span> input recordings along with the <span class="math notranslate nohighlight">\(K\)</span> parameters of a mixing console for each input selected by an audio engineer that will produce a mix of the recordings, we could design a model that learns to estimate these parameters computing a loss in the parameter space. This formulation is shown in Figure []b, where a model <span class="math notranslate nohighlight">\(f_\theta\)</span> is trained to estimate control parameters <span class="math notranslate nohighlight">\(\hat{\mathbf{P}}\)</span>. The weights of the network <span class="math notranslate nohighlight">\(\theta\)</span> are then trained according to a loss function <span class="math notranslate nohighlight">\(\mathcal{L}_p(\hat{\mathbf{P}}, \mathbf{P})\)</span> that measures the distance (e.g. mean squared error) between the estimated parameters and the ground truth parameters from the dataset. While this is a potentially desirable formulation since it provides a straightforward training setup, it is problematic for two main reasons.</p>
<p><em>Datasets</em>: Foremost is the reality that data of this form is not widely available. While it is possible to collect data of this form, it is challenging due to the diversity of the tools utilized by audio engineers. We could export the settings that describe the mix from digital audio workstation projects, but due to the wide range of digital audio workstations and the audio effect implementations present (plugins), defining a singular ontology onto which all mixing parameters can be mapped will require significant effort and is likely to face many edge cases.</p>
<p>For example, consider an equaliser effect. This effect is commonly used, however there are many different implementations of an equaliser. Some implementations may feature <strong>CONTINUE</strong>. Therefore, the only route towards constructing a dataset of this form would likely require defining a fixed mixing chain and asking audio engineers to use this to generate training data. However, this results in a significant compromise in data realism since audio engineers often utilize their preferred set of audio effects in different contexts and forcing the use of not only specific effect implementations but also a specific signal processing further limits the data realism.</p>
<p><em>Overparameterization:</em> In addition, another consideration for parameter estimation approach is the potential overparameterization of the audio effects and mixing console. An audio effect or signal processing device is overparameterized when there exists many different configurations in the parameter space of the device that result in outputs that are very close in the audio domain. This could be close in the audio domain with respect to an established objective metric, such as the time domain error or spectral distance, but also in the perceptual space, which is significantly harder to measure, but more likely to cause effects to be overparameterized. This is an issue when using a loss in the parameter space since it is possible for the model to generate parameter predictions that produce a mix close to or similar to the ground truth mix from the dataset but are far apart in the parameter space, and hence penalized during training.</p>
</section>
<section id="audio-loss">
<h3>Audio Loss<a class="headerlink" href="#audio-loss" title="Permalink to this headline">#</a></h3>
<p>As a result, existing parameter estimation systems for automatic mixing generally utilize a dataset that contains only input recordings and the stereo mix created by an audio engineer, without any knowledge of the underlying effect implementation or parameter configurations that were used. This approach uses the same dataset formulation as in the direct transformation approach, which addresses the limitations of the parameter space loss, however it introduces a</p>
<p>Training then involves that process of learning to map the input recordings to a mix that is as close as possible to the target mix in the dataset according to a predefined error metric <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y)\)</span>. Often this error metric is the <span class="math notranslate nohighlight">\(L_1\)</span> or <span class="math notranslate nohighlight">\(L_2\)</span> distance between the mixes in the time domain, or the distance between the magnitude spectrum of the mixes. We will discuss loss functions further in Section 2.4.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./part_2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="01_imp.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Intelligent Music Production</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="03_methods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Methods</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Christian Steinmetz, Soumya Sai Vanka, Marco Martínez, Gary Bromham<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>